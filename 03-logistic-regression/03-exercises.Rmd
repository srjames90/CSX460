---
title: "03-exercises"
author: "Steven Rivera-James"
date: "April 20, 2016"
output: html_document
---

## Readings

***APM***

- Chapter 4 "Over Fitting and Model Tuning"
- Chapter 12.2 "Logisitic Regression""


## Miscellaneous

I am still struggling with names ...

- Please send me your picture


## Assignment 

Note: The following will set-up your environment for this exercise. If you get an error stating that the packages have not been found, you need to install those packages.


```{r,echo=FALSE, warning=FALSE, message=FALSE}

packs <-  c('AppliedPredictiveModeling', 'ggplot2', 'magrittr', 'dplyr', 'caret', 'MASS', 'C50' , 'gmodels')

for( nm in packs ) { 
  # message(nm)
  if( ! nm  %in% installed.packages()[,1]  ) install.packages(nm)
  library(nm, character.only = TRUE)
}

# Load data set into environment
data(FuelEconomy)     # See ?cars2010
fe <- dplyr::bind_rows(cars2010, cars2011, cars2012)    # Define Da


data("GermanCredit")  # see GermanCredit

... = NULL  # Needed for aesthetics 

```


## StepAIC


Using Fuel Economy data set from the **AppliedPredictiveModeling** Package.
- fit the simplest possible model using lm
- Use MASS::StepAIC to improve the model using forward stepwise regression
- Fit the "full" model using lm
- USe MASS::StepAIC to improve the model using backward stepwise regression 

```{r}
form = ~ EngDispl + NumCyl + Transmission + FE + AirAspirationMethod + NumGears + TransLockup + TransCreeperGear + DriveDesc + IntakeValvePerCyl + ExhaustValvesPerCyl + CarlineClassDesc + VarValveTiming + VarValveLift

#simplest model, the natural prediction, the mean of FE
fit.min <- lm(FE ~ 1, fe)

#Using stepAic to improve model using forward stepwise 
fit.min.forward <- stepAIC(fit.min, scope = form, direction = "forward", trace = 0)
summary(fit.min.forward)

#Fit the full model, all variables
fit.full = lm(FE ~ . , fe)

#Use backward stepwise regression on the full model
fit.full.back <- stepAIC(fit.full, scope = ~1, direction = "backward", trace = 0)
summary(fit.full.back)

#loss functions from 2nd assignment. #Using these to evaluate our models
rmse <- function(y,yhat) {
  (y - yhat)^2 %>% mean %>% sqrt
}

mae <- function(y, yhat) {
  (y - yhat) %>% abs %>% mean
}

medae <- function(y, yhat) {
  (y - yhat) %>% abs %>% median
}

pred.forward <- predict(fit.min.forward, fe)
pred.back <- predict(fit.full.back, fe)

#errors for forward
rmse(fe$FE, pred.forward)
mae(fe$FE, pred.forward)
medae(fe$FE, pred.forward)

#errors for backward
rmse(fe$FE, pred.back)
mae(fe$FE, pred.back)
medae(fe$FE, pred.back)
```

- Are they the same model? If not why?  Which is better?

One can argue they are the same given the differences are marginal and the residuals/coefficients are  practically the same. Forward started with the intercept and added variables as it saw fit while backwards started with the full model and removed. The end result were 2 similar models: specifically, backwards took into account variables that forward didn't (e.g., AirAspirationMethod, TransCreeperGear, etc.) and ignored IntakeValveCyl.
If one were to argue that they are different, then backwards would be better since the errors for backward are less than those for forward, though marginally so.
Note: Given that both models used the full set of data rather than separating between training data and testing data, I'd say that these models are overfit.

## Logsitic and Inverse Logistic Transformation 

- Write an R function for the logistic function. The function should accept a `numeric` vector with values `[-Inf,Inf]` and produce a numeric vector in the the range `[0,1]`.

- Plot the logistic function from  `[-10,10]`

- Write a R function for the inverse logistic function. The function should accept a `numeric` vector with values `[0,1]` and prodcuce a numeric vector in the range `[-Inf,Inf]`

- Plot the Inverse Logistic function from `[0,1]`


**Hint:** For plotting curves see `?graphics::curve` or `?ggplot2::stat_function`


```{r}

logistic <- function(x) { 
  1/(1+exp(-x))
}

#plot using curve from hint above
curve(logistic, -10, 10)

logistic_inv <- function(y) { 
  -log((1/y) - 1)  
}

#plot using curve from hint above
curve(logistic_inv, 0, 1)

```

**NOTE"** These functions are quite handy, in evaluating logistic regression results. You may want to save these functions in your own package.  

```{r}
# DO NOT EDIT
c(-Inf,0,Inf) %>% logistic

c(0,0.5,1) %>% logistic_inv

```


## German Credit Model

Using the GermanCredit data from the **Caret** package/ UCI Machine Learning Library, create a model for `Class` ("Good" vs. "Bad" ). Show your model performance.  

```{r}
data(GermanCredit)

#set seed
set.seed(123)
credit.tsample <- sample(1000, 900)

#set our training data and our testing data
credit.train <- GermanCredit[credit.tsample,]
credit.test <- GermanCredit[-credit.tsample,]

#cost matrix
#matrix.dimensions <- list(c('Bad', 'Good'), c('Bad', 'Good'))
#names(matrix.dimensions) <- c('predicted', 'actual')

#Note: Using the cost matrix is good at detecting false-positives, but errors are greater..might be worth it

#decision tree model with boosting
#Excluding the actual decision (reserved for target)
credit.treemodel <- C5.0(credit.train[-10], credit.train$Class, trials = 10)

#predict
credit.pred <- predict(credit.treemodel, credit.test)

#Show cross table to see accuracy
CrossTable(credit.test$Class, credit.pred, prop.chisq = F, prop.c = F, prop.r = F,dnn=c('actual class', 'predicted class'))

#Model using glm
#not splitting betwen test and training here
credit.glm <-glm(formula = Class ~ ., family  = 'binomial', data = credit.train)

credit.glm.backward <-stepAIC(credit.glm, data = credit.train, direction = 'backward', trace = 0)

summary(credit.glm.backward)

#performs well, way better than decision tree in regards to false positives
credit.glm.pred <- predict.glm(credit.glm.backward, credit.test)
credit.glm.predClass <- ifelse(credit.glm.pred > .8, 'Good', 'Bad')
CrossTable(credit.test$Class, credit.glm.predClass, prop.chisq = F, prop.c = F, prop.r = F,dnn=c('actual class', 'predicted class'))
```



## Iterative Correlated Feature Removal 

- Implement Kuhn's iterative feature removal function described in **APM** Section 3.5, page 47

```{r}
#untested
#takes a correlation matrix, then returns a new correlation matrix.
#can use colnames for features
feature_removal <- function(cor_matrix, threshold = .8) {
  if(length(cor_matrix == 0))
    return()
  #absolute for initial call
  abscor_matrix <- abs(cor_matrix)
  #0 out the diagonal because self-correlation will be highest
  diag(abscor_matrix) <- 0
  #if below threshold return the matrix
  if(max(abscor_matrix) <= threshold)
    return(cor_matrix)
  #find pairs whose abs correlation is equal to the max. Use first max.
  max <- which(max(abscor_matrix) == abscor_matrix, arr.ind = T)[1,]
  #Get A and B
  a = max[[1]]
  b = max[[2]]
  #Compute the averages
  a_mean = mean(abscor_matrix[,-b][a,])
  b_mean = mean(abscor_matrix[-a,][,b])
  if(a_mean > b_mean) {
    return(feature_removal(cor_matrix[-a,-a]))
  } else {
    return(feature_removal(cor_matrix[-b,-b]))
  }
  
}

#untested iterative version of feature removal
feature_removal_iter <- function(cor_matrix, threshold = .8) {
  while (length(cor_matrix) > 0 && max(abs(cor_matrix)) > threshold) {
    #find pairs whose abs correlation is equal to the max. Use first max.
    max <- which(max(abs(cor_matrix)) == abs(matrix), arr.ind = T)[1,]
    #Get A and B
    a = max[[1]]
    b = max[[2]]
    #Compute the averages
    a_mean = mean(abs(cor_matrix[,-b][a,]))
    b_mean = mean(abs(cor_matrix[-a,][,b]))
    if(a_mean > b_mean) {
      cor_matrix[-a,-a]
    } else {
      cor_matrix[-b,-b]
    }
  }
  cor_matrix
}
```

## Synthetic Data (Optional)

Sometimes it is useful to "synthesize" feature data for to understand how a certain model behaves. 
Sythesize the following features 1000-element vectors: 

- x1: a normally distributed variable with `mean = 20` and standard deviation = 20 (`sd=8`).
- x2: a log-normally distributed feature with `meanlog = 1`, `sdlog=1.2`
- x3: a uniformly distributed feature with `min=0` and `max=50`. 

```{r}
nsamples = 20

x1 <- rnorm(nsamples,20,20)  
x2 <- rlnorm(nsamples, meanlog=1, sdlog = 1.2)
x3 <- runif(nsamples,0,50)

```

Next synthesis a response, `y` using the betas provided and an intercept that is normally distributed at 20 with standard deviation of 2. (**Hint:**  The betas thought of can be a vector or matrix)



```{r}

beta0 <- rnorm(nsamples,0,15)  # intercept!
beta1 <- 2.3
beta2 <- 4
beta3 <- 7

betas <- matrix( c(2.5, 4, 7), nrow=1  )  # 1x4 matrix

# x0 <- rep(1,nsamples) 

X  <- cbind(x1,x2,x3)  # 1000x4

y <- betas %*% t(X) %>% t
y <- y + beta0

qplot(y)
dat <- data.frame(y,X)

fit <- lm( y ~ . , dat )

coef(fit)

fit
```

- Did you recover the betas? 
- Is the model good?
- What happens if increase the value of `nsamples`? Decrease it?
- What transformations would you apply to x1? x2? x3? 

