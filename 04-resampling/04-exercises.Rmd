---
title: "Sensitivity and Specificity"
author: "Steven Rivera"
date: "October 5, 2015"
output: html_document
---


## Readings

***APM***

- ***Chapter 5 Measuring Performance in Regression Models*** (esp. ***5.2 The Variance Bias Trade-Off***)  (5 pages)
- ***Chapter 11 Measuring Performance in Classification Models*** (~20 pages)
- ***Chapter 7.4 K-Nearest Neighbors (regression)*** (2 pages)
- ***Chapter 13.5 K-Nearest Neighbors (classification)*** (3 pages)


```{r, echo=FALSE, results='hide', warning=FALSE }
packs <-  c('ggplot2', 'magrittr', 'dplyr', 'caret', 'AppliedPredictiveModeling')

for( nm in packs ) { 
  # message(nm)
  if( ! nm  %in% installed.packages()[,1]  ) install.packages(nm)
  library(nm, character.only = TRUE)
}

.. = NULL  # For Aesthetics

```


## EXERCISE 1: Resampling

`x` is a random variable. We want to not only know what the `mean(x)` is but want to calculate the uncertainty of `mean(x)`.  Measuring the uncertainty requires repeated measurements of `mean(x)`.

- Calculate the mean of `x`.
- Calculte the `sd( mean(x) )` using the **using 10-fold cross-validation**.  Create your own folds, show your work. (An example is for the Bootstrap is given as a hint. )


```{r}
set.seed(1) 
x <- runif(20,1,20)

x_mean = mean(x)

k=10

#randomize the vector
x <- x[order(rnorm(20))]

# CROSS-VALIDATION

sd_cv <- sapply(seq(1, 20, by = 2), function(i) {x[-i][-i]} %>% mean) %>% sd


# BOOTSTRAP (EXAMPLE)
sd_boot <- sapply(1:k, function(i) sample(x,replace=TRUE) %>% mean ) %>% sd

```


- sd_cv   is: `r sd_cv`
- sd_boot is: `r sd_boot`



# Exercise 2: Binomial Metrics

Here's a really simple Model of Versicolor iris based on the **iris** data :

```{r}
set.seed(1)
data(iris)

qplot( data=iris, x=Petal.Length, y=Sepal.Length, color=Species )

# Create Dependent Variable
iris$Versicolor <- 
  ifelse( iris$Species == 'versicolor', "versicolor", "other" ) %>% as.factor
iris$Species = NULL 

wh <- sample.int( nrow(iris), size=nrow(iris)/2 )
train <- iris[ wh,]
test <- iris[ -wh, ]


fit.glm <- glm( Versicolor ~ . - Sepal.Length, data=train, family=binomial )
```


Use the models to and write functions to calculate:

* Prevalence 
* Accuracy
* Error Rate / Misclassification Rate
* True Positive Rate  
* False Positive Rate
* True Negative Rate  
* False Negative Rate 
* Sensitivity 
* Specificity 
* Recall 
* Precision

The functions should take two logical vectors of the same length, `y` and `yhat`

```{r}

prevalence = function(y,yhat) {sum(y) / length(y)}
accuracy   =  function(y, yhat) {sum(y == yhat) / length(y)} 
error_rate = function(y, yhat) {1 - accuracy(y,yhat)}
tpr = function(y, yhat) {sum(y & yhat) / sum(y)}
fpr = function(y, yhat) {sum(yhat & !y) / sum(!y)}
tnr = function(y, yhat) {sum(!y & !yhat) / sum(!y)}
sensitivity = tpr
specificity = tnr
recall = tpr 
precision = function(y, yhat) {sum(y & yhat) / sum(yhat)}

# EXAMPLE: fpr
# The FPR is THE NUMBER OF FALSE POSITIVES / NEGATIVES (TN+FP)

threshold = 0.5 
y = test$Versicolor == 'versicolor'
yhat = predict(fit.glm, test, type="response") > threshold

fpr(y,yhat)
prevalence(y, yhat)
accuracy(y,yhat)
error_rate(y, yhat)
tpr(y,yhat)
tnr(y,yhat)
sensitivity(y,yhat)
specificity(y,yhat)
recall(y,yhat)
precision(y, yhat)

```

- What is wrong with the modeling approach used?

Though in some cases, the accuracy and and true negative rate are okay, the true positive rate and false postive rate aren't great, especially if this data set/customer were sensitive to these rates e.g, in the case of credit data.



