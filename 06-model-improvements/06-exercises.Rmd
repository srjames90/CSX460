---
title: "05-exercises"
author: "Steven Rivera-James"
date: "2016-05-xx"
output: html_document
---

## Reading:
- **APM** Chapter 8.6 and 8.8 
- **APM** Chapter 14.8 
- **APM** Chapter 7.1 & 7.3 "Non-Linear Regression Models"
- **APM** Chapter 13.2 & 13.4 "Non-Linear Classifcation Models"


```{r,echo=FALSE}

packs <-  c('AppliedPredictiveModeling', 'ggplot2', 'magrittr', 'dplyr', 'caret', 'ipred', 'bst')

for( nm in packs ) { 
  # message(nm)
  if( ! nm  %in% installed.packages()[,1]  ) install.packages(nm)
  library(nm, character.only = TRUE)
}

# Load data set into environment
data(FuelEconomy)
.. = NULL  # Needed for aesthetics 

FE <- dplyr::bind_rows(cars2010, cars2011, cars2012)    # Define Da

```

## Fuel Economy 


This week we return to the Fuel Economy Data having learned much about model building. This assignment is to go through the process of building several regression models and pick the most predictive model. Use the `FE` data set created for you above.


Start by making choosing a metric and making a naive guess of model performance: 

Metric: Root Mean Squared
Naive Guess: 35.03823 (Mean)
Expected Model Performance (based on Naive Guess): 8.096176

Show your work below for the calculations

```{r} 

  
naive_guess = mean(FE$FE)

err_naive_guess = (FE$FE - naive_guess)^2 %>% mean %>% sqrt

```


Based only your intuition, how low do your think you can get your metric: I'd say that we can get this down to .5 or .4 


## Examine your data

 * Plot your response/outcome 

 * Make a guess of a strong predictor: EngDispl 
 * Plot your response vs your predictor. 

```{r}

ggplot(data=FE, aes(x = EngDispl, y=FE)) + geom_point() + geom_hline(yintercept=naive_guess, color='red')

```



## Build Simple Models

Using **caret**, build a simple linear model and a simple tree model. 

```{r}
seed <- 123

control <- trainControl(number=5)

set.seed(seed)
fit.lm <- train(FE~., data=FE, method='lm', trControl=control)

set.seed(seed)
fit.rp <- train(FE ~ ., data=FE, method='rpart', trControl=control, tuneLength=20)

```


What did you learn about the data from these models.

Both models reduce our error significantly, while still using all the predictors; however, the linear model slighty reduces the error more than the tree.

## Build More Advanced Models

Now refine your models. Use **caret** to build advanced models:
- one that uses model averaging (bagging) 
- one that uses boosting 

```{r}

# Your work here.
set.seed(seed)
fit.bag   <-  train(FE ~ ., data=FE, method='treebag', trControl=control)

set.seed(seed)
fit.boost <- train(FE ~ ., data=FE, method='BstLm', trControl=control, tuneLength=20)

fit.lm
fit.rp

fit.boost
fit.bag

```


## Conclusion 

Which model would you use and why?  Under different circumstances why would you choose one of the other models.

Amongst the advanced models, I would choose the boosted linear model due to a lower error and comparable Rsquared (similar to the comparison of the simple linear model and the tree); however, if I were to choose amongst all the models, I'd choose the simple linear model given that it performed the best, not to mention that it is the simplest. My choices for advanced models might not have been the best, though, given that they are the boosted and bagged versions of the simpler models; however, I wanted to choose models that were of the same vein for comparison reasons. 

There are many circumstances where I'd want to use the advanced models. Even in this case I might if I consider how limited my comparisons are. For instance, maybe with a larger dataset and a larger bootstrap (from what I've gathered more tends to work better), the latter may prove more useful, though computationally more expensive.

